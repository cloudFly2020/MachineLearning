{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    \n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], \n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]#[0,0,1,1,1......]\n",
    "    classVec = [0, 1, 0, 1, 0, 1] \n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    #怎么创建单词集合\n",
    "    #利用集合创建唯一单词列表\n",
    "    vocabList=set([])\n",
    "    for line in dataSet:\n",
    "        #vocabList=vocabList|line\n",
    "        vocabList=vocabList|set(line)\n",
    "    return list(vocabList)###################list()不能拉下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWord2Vec(vocabList,dataLine):\n",
    "    m=len(vocabList)\n",
    "    #returnVec=np.zeros((m,1))\n",
    "    returnVec=[0]*m  #忘了\n",
    "    for word in dataLine:\n",
    "        if word in vocabList:### 如何计数 # 创建一个和 唯一词汇表等长的向量，并将其元素都设置为0 \n",
    "            #如何让对应位置加1 取出单词在vocablist 索引 就是在returnVec 的索引 另其值为1\n",
    "            #returnVec.index(vocalList.index(word))=1\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:\n",
    "            print('the word %s is not in vocablist ' %word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainB0(dataMat,labels):\n",
    "    #1.计算侮辱性类别在总类别中出现的概率 侮辱性类别的个数sum(trainCategory) 0+1+1+0\n",
    "    numVocab=len(dataMat[0])\n",
    "    #pAb=sum(labels)/float(len(dataMat))           # float()\n",
    "    pAb=np.sum(labels)/float(numVocab) \n",
    "    #2.定义各类别下 单词出现次数向量 长度是总单词数 定义各类别下单词总数\n",
    "    \n",
    "    #p0Num=[1]*numVocab\n",
    "    p0Num=np.ones(numVocab)\n",
    "    p1Num=np.ones(numVocab)\n",
    "    print('type of dataMat',type(dataMat))\n",
    "    print('type of ponum',type(p0Num))\n",
    "    p1Denom=2.0\n",
    "    p0Denom=2.0\n",
    "    \n",
    "    #for i in dataMat:\n",
    "    for i in range(len(dataMat)):\n",
    "        if labels[i]==1:\n",
    "            p1Num+=dataMat[i]\n",
    "            #p1Denom+=sum(dataMat[i])\n",
    "            p1Denom+=np.sum(dataMat[i])\n",
    "        else:\n",
    "            p0Num+=dataMat[i]\n",
    "            p0Denom+=np.sum(dataMat[i])##################p1Denom+=np.sum(dataMat[i]) 复制成p1\n",
    "    p1V=np.log(p1Num/p1Denom)              #不能忘log()\n",
    "    p0V=np.log(p0Num/p0Denom)################np.log() 可以避免下溢出（多个很小数相乘为0），和浮点数四舍五入导致的错误\n",
    "    print('type of p0v',type(p0V))\n",
    "    print('p1V',p1V,'\\n p0V',p0V,'\\n pAb',pAb)\n",
    "    return p0V,p1V,pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test,p0V,p1V,pAb):\n",
    "    # 测试数据每个词与其对应的概率相关联起来 相加 比较大小  1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为\n",
    "    print('classify test type',type(test),'p0V test',type(p0V))\n",
    "    p1=sum(test*p1V)+np.log(pAb)                           ######np.log 不要忘了\n",
    "    p0=sum(test*p0V)+np.log(1.0-pAb)\n",
    "    print('p1',p1,'----p0',p0)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes():\n",
    "    #1. 加载数据集\n",
    "    dataList,dataLabels=loadDataSet()\n",
    "    #2 创建不重复单词集合\n",
    "    vocabList=createVocabList(dataList)\n",
    "    # 3 计算单词是否出现 创建每行数据各单词出现次数 矩阵\n",
    "    trainMat=[]\n",
    "    for line in dataList:\n",
    "        trainMat.append(setOfWord2Vec(vocabList,line))\n",
    "    #4 训练数据得出条件概率\n",
    "    p0V,p1V,pAb=trainB0(np.array(trainMat),np.array(dataLabels))#构造函数 算出 p(c=1)类别的概率 1类别下各单词出现概率向量 0类别下 各单词出现的概率向量\n",
    "    #有了条件概率 测试数据\n",
    "    #5 测试数据 统计测试数据各单词出现的次数向量 构造分类函数\n",
    "    test=['love', 'my', 'dalmation']\n",
    "    testDoc=np.array(setOfWord2Vec(vocabList,test))\n",
    "    # 分类函数 传入条件概率 类别概率\n",
    "    label=classify(testDoc,p0V,p1V,pAb)\n",
    "    print('the test result is: {}'.format(label))    \n",
    "    test_two = ['stupid', 'garbage']\n",
    "    test_two_doc = np.array(setOfWord2Vec(vocabList, test_two))\n",
    "    print('the second test  result is: {}'.format(classify(test_two_doc,p0V,p1V,pAb)))       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of dataMat <class 'numpy.ndarray'>\n",
      "type of ponum <class 'numpy.ndarray'>\n",
      "type of p0v <class 'numpy.ndarray'>\n",
      "p1V [-3.04452244 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -3.04452244 -3.04452244 -3.04452244 -3.04452244\n",
      " -2.35137526 -3.04452244 -1.94591015 -3.04452244 -2.35137526 -3.04452244\n",
      " -2.35137526 -1.65822808 -2.35137526 -2.35137526 -2.35137526 -2.35137526\n",
      " -3.04452244 -2.35137526 -2.35137526 -1.94591015 -2.35137526 -3.04452244\n",
      " -2.35137526 -3.04452244] \n",
      " p0V [-2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -3.25809654 -2.56494936 -3.25809654 -1.87180218 -3.25809654 -2.56494936\n",
      " -3.25809654 -3.25809654 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.15948425 -2.56494936] \n",
      " pAb 0.09375\n",
      "p1 -11.500690927301886 ----p0 -7.100140964637918\n",
      "the test result is: 0\n",
      "p1 -6.376726947898627 ----p0 -6.614633148856217\n",
      "the second test  result is: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    test_naive_bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "[[1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "dataList,dataLabels=loadDataSet()\n",
    "vocabList=createVocabList(dataList)\n",
    "trainMat=[]\n",
    "for line in dataList:\n",
    "    print(line)\n",
    "    trainMat.append(setOfWord2Vec(vocabList,line))\n",
    "print(trainMat)\n",
    "#print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_parse(text):\n",
    "    '''\n",
    "    Desc:\n",
    "        词条切分\n",
    "    param:\n",
    "        传入词条\n",
    "    return:\n",
    "        单词列表\n",
    "    '''\n",
    "    import re\n",
    "    word_list=re.split(r'\\W+',text)\n",
    "    if len(word_list)==0:\n",
    "        print(word_list)\n",
    "    result=[word.lower() for word in word_list if len(word)>2]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_word2Vec(vocabList,docLine):\n",
    "    word_count=[0]*len(vocabList)\n",
    "    for word in docLine:\n",
    "        if word in vocabList:\n",
    "            #word_count[word]+=1\n",
    "            #word_count[docLine.index(word)]+=1\n",
    "            word_count[vocabList.index(word)]+=1#######唯一词列表中取索引加1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def span_test():\n",
    "    '''\n",
    "    Desc:\n",
    "        用贝叶斯对垃圾邮件分类\n",
    "    param:\n",
    "        None\n",
    "    return:\n",
    "        nothing\n",
    "    '''\n",
    "    doc_list=[]\n",
    "    class_list=[]\n",
    "    full_text=[]\n",
    "    for i in range(1,26):\n",
    "        try:\n",
    "            spam_word=text_parse(open('E:/github/MachineLearning/input/4.NaiveBayes/email/spam/{}.txt'.format(i)).read())\n",
    "        except:\n",
    "            spam_word=text_parse(open('E:/github/MachineLearning/input/4.NaiveBayes/email/spam/{}.txt'.format(i),encoding='Windows 1252').read())\n",
    "        doc_list.append(spam_word)\n",
    "        full_text.extend(spam_word)\n",
    "        class_list.append(1)\n",
    "        \n",
    "        try:\n",
    "            ham_word=text_parse(open('E:/github/MachineLearning/input/4.NaiveBayes/email/ham/{}.txt'.format(i)).read())\n",
    "        except:\n",
    "            ham_word=text_parse(open('E:/github/MachineLearning/input/4.NaiveBayes/email/ham/{}.txt'.format(i),encoding='Windows 1252').read())\n",
    "        doc_list.append(ham_word)\n",
    "        full_text.extend(ham_word)\n",
    "        class_list.append(0)\n",
    "        \n",
    "       \n",
    "        #print(doc_list)\n",
    "        \n",
    "#         print('---------------')\n",
    "#         print(len(full_text))\n",
    "#         print(full_text)\n",
    "#         print('---------------')\n",
    "#         print(class_list)\n",
    "#         break\n",
    "    #print('full_text',full_text) #########################参数不要传错了\n",
    "    vocabList=createVocabList(doc_list)\n",
    "    #print('vocabList',vocabList)\n",
    "    #在50个数随机取10个 这10个做训练数据 50个中删除10个剩下的40个做测试\n",
    "    \n",
    "    #calc_most_freq(vocabList,full_text)\n",
    "    trainingSet=range(50)\n",
    "    #trainingSet=[]\n",
    "    testSet=[]\n",
    "#     for i in range(10):## random.uniform(x, y) 随机生成 一 个范围为 x ~ y 的实数\n",
    "#         testIndex=int(np.random.uniform(0,50))        ############## int() 不要忘了\n",
    "#         testSet.append(trainingSet[testIndex])\n",
    "#         del(trainingSet[testIndex])    ####'range' object doesn't support item deletion\n",
    "    import random \n",
    "    #testSet=[num for num in random.sample(50,10)]\n",
    "    testSet=[int(num) for num in random.sample(range(50),10)]\n",
    "    trainingSet=list(set(trainingSet)-set(testSet))\n",
    "    trainMat=[]\n",
    "    trainClass=[]\n",
    "    \n",
    "    errorCount=0.0\n",
    "    for i in trainingSet:\n",
    "        trainMat.append(bag_word2Vec(vocabList,doc_list[i]))\n",
    "        trainClass.append(class_list[i])\n",
    "    #print('trainMat',trainMat)\n",
    "    #p0V,p1V,pAb=trainB0(trainMat,trainClass)\n",
    "    p0V,p1V,pAb=trainB0(np.array(trainMat),np.array(trainClass))\n",
    "    for i in testSet:\n",
    "        testLine=bag_word2Vec(vocabList,doc_list[i])\n",
    "        #testClass.append(class_list[i])\n",
    "        #print('\\n testLine',testLine)\n",
    "        label=classify(np.array(testLine),p0V,p1V,pAb)\n",
    "        if label !=class_list[i]:\n",
    "            errorCount+=1\n",
    "            print('classify is %s ,the real label is %s'%(label,class_list[i]))\n",
    "    errorRatio=errorCount/float(len(testSet))\n",
    "    print('error ratio is',errorRatio)\n",
    "    #return errorRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n",
      "top30 [('the', 50), ('and', 46), ('you', 44), ('100', 28), ('for', 23), ('your', 20), ('that', 17), ('have', 15), ('this', 15), ('yourpenis', 14), ('increase', 14), ('from', 13), ('google', 12), ('with', 11), ('are', 11), ('buy', 11), ('can', 11), ('15mg', 10), ('pills', 10), ('online', 10), ('will', 10), ('answer', 10), ('codeine', 9), ('only', 9), ('more', 9), ('safe', 9), ('all', 9), ('inches', 8), ('experience', 8), ('designed', 8)]\n",
      "vocab {'the': 50, 'and': 46, 'you': 44, '100': 28, 'for': 23, 'your': 20, 'that': 17, 'have': 15, 'this': 15, 'yourpenis': 14, 'increase': 14, 'from': 13, 'google': 12, 'with': 11, 'are': 11, 'buy': 11, 'can': 11, '15mg': 10, 'pills': 10, 'online': 10, 'will': 10, 'answer': 10, 'codeine': 9, 'only': 9, 'more': 9, 'safe': 9, 'all': 9, 'inches': 8, 'experience': 8, 'designed': 8}\n",
      "type of dataMat <class 'numpy.ndarray'>\n",
      "type of ponum <class 'numpy.ndarray'>\n",
      "type of p0v <class 'numpy.ndarray'>\n",
      "p1V [-6.50279005 -6.50279005 -6.50279005 -5.80964287 -5.80964287 -3.79473984\n",
      " -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.40417776 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.40417776\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.40417776 -4.5568799  -4.4233485\n",
      " -4.5568799  -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005\n",
      " -6.50279005 -4.4233485  -4.10489477 -6.50279005 -4.4233485  -6.50279005\n",
      " -6.50279005 -6.50279005 -5.80964287 -5.40417776 -6.50279005 -5.40417776\n",
      " -6.50279005 -6.50279005 -5.11649568 -5.40417776 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -5.40417776\n",
      " -6.50279005 -4.89335213 -6.50279005 -5.11649568 -5.80964287 -4.71103058\n",
      " -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -4.10489477 -5.11649568\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -5.80964287 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.11649568 -5.80964287\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.40417776 -5.40417776\n",
      " -3.61241829 -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.11649568\n",
      " -6.50279005 -5.40417776 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -4.5568799  -5.80964287 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.40417776 -6.50279005 -6.50279005 -6.50279005 -5.40417776 -5.80964287\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -4.5568799\n",
      " -6.50279005 -6.50279005 -4.71103058 -5.40417776 -5.80964287 -6.50279005\n",
      " -5.11649568 -6.50279005 -6.50279005 -5.40417776 -4.4233485  -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.40417776 -5.40417776 -5.40417776\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.80964287\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.80964287 -6.50279005 -5.40417776\n",
      " -4.4233485  -5.80964287 -4.4233485  -6.50279005 -6.50279005 -6.50279005\n",
      " -5.40417776 -5.40417776 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.80964287 -4.71103058 -6.50279005 -5.40417776 -6.50279005 -6.50279005\n",
      " -6.50279005 -4.4233485  -6.50279005 -5.40417776 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.40417776 -6.50279005\n",
      " -6.50279005 -4.4233485  -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.80964287\n",
      " -6.50279005 -5.40417776 -6.50279005 -5.40417776 -6.50279005 -5.40417776\n",
      " -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.40417776 -6.50279005 -6.50279005 -4.89335213 -5.40417776\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.40417776 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -4.30556547 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.80964287 -6.50279005 -4.4233485  -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.80964287 -4.89335213 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.80964287 -6.50279005\n",
      " -5.40417776 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.40417776 -5.40417776 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -5.40417776 -5.40417776 -5.40417776\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.40417776 -5.80964287 -5.80964287\n",
      " -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -4.4233485  -6.50279005 -5.40417776\n",
      " -4.89335213 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -4.4233485\n",
      " -5.40417776 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -5.40417776\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.11649568 -4.4233485\n",
      " -6.50279005 -5.80964287 -5.40417776 -5.40417776 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -5.80964287 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -4.4233485  -5.80964287 -5.40417776\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.40417776 -6.50279005 -5.80964287\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.80964287\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.40417776 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.40417776 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -5.80964287 -6.50279005 -6.50279005\n",
      " -6.50279005 -3.79473984 -5.40417776 -4.89335213 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.80964287\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.40417776 -5.40417776\n",
      " -5.80964287 -6.50279005 -6.50279005 -4.4233485  -6.50279005 -6.50279005\n",
      " -5.40417776 -5.40417776 -6.50279005 -5.80964287 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005 -5.80964287\n",
      " -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.80964287 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.80964287 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.40417776\n",
      " -5.40417776 -5.80964287 -6.50279005 -5.80964287 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -4.4233485\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.40417776\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.40417776 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -4.4233485\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.40417776\n",
      " -5.80964287 -4.4233485  -5.40417776 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -6.50279005 -5.40417776 -5.80964287\n",
      " -5.80964287 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.40417776\n",
      " -6.50279005 -4.5568799  -5.40417776 -5.40417776 -5.40417776 -6.50279005\n",
      " -6.50279005 -4.89335213 -6.50279005 -6.50279005 -5.11649568 -6.50279005\n",
      " -6.50279005 -6.50279005 -4.4233485  -4.4233485  -5.40417776 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -4.5568799  -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -5.80964287 -6.50279005 -6.50279005\n",
      " -4.89335213 -4.30556547 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -5.80964287 -5.40417776 -6.50279005\n",
      " -5.80964287 -4.10489477 -6.50279005 -6.50279005 -4.89335213 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.80964287 -5.40417776 -6.50279005 -6.50279005\n",
      " -5.40417776 -5.40417776 -6.50279005 -4.4233485  -4.20020495 -4.5568799\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.80964287\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -5.80964287 -5.80964287\n",
      " -6.50279005 -4.20020495 -6.50279005 -5.80964287 -6.50279005 -4.4233485\n",
      " -4.20020495 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -3.17058554 -5.80964287 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -5.40417776 -5.80964287\n",
      " -5.40417776 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.40417776 -5.80964287 -6.50279005 -5.11649568 -6.50279005\n",
      " -5.80964287 -6.50279005 -5.40417776 -5.80964287 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.40417776 -6.50279005 -6.50279005 -5.80964287 -5.80964287\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -4.4233485  -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -6.50279005 -5.80964287 -6.50279005 -6.50279005 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -6.50279005 -6.50279005 -6.50279005\n",
      " -6.50279005 -5.80964287 -6.50279005 -5.80964287 -6.50279005 -6.50279005\n",
      " -5.80964287 -6.50279005 -6.50279005 -6.50279005 -5.80964287 -6.50279005\n",
      " -6.50279005 -6.50279005 -6.50279005 -6.50279005] \n",
      " p0V [-5.64662406 -5.64662406 -5.35894199 -6.74523635 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.74523635 -5.13579844 -6.05208917 -6.74523635 -6.74523635 -6.74523635\n",
      " -5.64662406 -6.05208917 -6.05208917 -6.74523635 -5.35894199 -6.05208917\n",
      " -6.05208917 -6.74523635 -3.37794052 -5.64662406 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.74523635 -6.74523635 -6.05208917 -5.64662406\n",
      " -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.05208917 -5.64662406 -5.35894199\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.05208917 -6.05208917 -5.13579844 -6.05208917 -6.74523635 -5.64662406\n",
      " -4.95347688 -5.13579844 -6.74523635 -5.64662406 -6.74523635 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.74523635\n",
      " -5.64662406 -5.64662406 -6.05208917 -6.05208917 -6.74523635 -6.74523635\n",
      " -3.56718252 -6.05208917 -6.74523635 -5.64662406 -6.05208917 -6.74523635\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.74523635 -6.05208917 -5.64662406 -5.35894199 -4.44265126\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.74523635 -6.74523635 -5.35894199\n",
      " -6.05208917 -4.34734108 -6.74523635 -6.74523635 -6.74523635 -6.74523635\n",
      " -2.98403623 -5.35894199 -6.05208917 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.74523635 -5.35894199 -5.64662406 -6.74523635 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -5.64662406 -6.05208917 -5.64662406 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -5.64662406 -6.05208917 -5.35894199 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.05208917\n",
      " -4.7993262  -4.7993262  -6.05208917 -6.05208917 -5.64662406 -5.13579844\n",
      " -6.74523635 -6.74523635 -4.34734108 -6.74523635 -5.64662406 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.74523635 -6.05208917\n",
      " -6.05208917 -6.74523635 -5.64662406 -6.05208917 -6.05208917 -5.35894199\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.74523635\n",
      " -6.05208917 -5.35894199 -6.74523635 -5.64662406 -6.74523635 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.05208917 -5.13579844 -6.74523635 -6.74523635\n",
      " -6.05208917 -5.64662406 -6.05208917 -6.05208917 -4.03718615 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.05208917 -5.64662406 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.05208917 -5.64662406 -5.64662406 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.05208917 -6.74523635 -5.64662406 -6.05208917 -6.74523635\n",
      " -6.05208917 -6.05208917 -5.64662406 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.05208917 -5.35894199 -4.7993262  -5.64662406\n",
      " -6.74523635 -6.74523635 -6.74523635 -5.13579844 -6.74523635 -6.05208917\n",
      " -5.64662406 -6.74523635 -6.05208917 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.74523635 -6.05208917 -5.64662406 -6.05208917 -5.64662406\n",
      " -6.74523635 -6.05208917 -5.64662406 -6.74523635 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.74523635 -5.13579844 -5.35894199 -6.05208917\n",
      " -5.35894199 -6.74523635 -6.05208917 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.74523635 -4.95347688 -5.64662406\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.74523635 -4.95347688 -5.35894199 -6.05208917\n",
      " -6.74523635 -6.05208917 -6.74523635 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.05208917 -5.64662406 -5.35894199 -6.74523635 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -5.35894199 -6.74523635 -5.64662406 -6.74523635\n",
      " -5.64662406 -6.05208917 -6.74523635 -5.13579844 -4.66579481 -6.74523635\n",
      " -5.13579844 -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.05208917 -5.64662406 -6.74523635\n",
      " -6.74523635 -6.05208917 -5.64662406 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.74523635 -5.64662406 -6.74523635 -6.05208917 -6.05208917 -6.74523635\n",
      " -5.35894199 -6.05208917 -5.64662406 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.05208917 -4.18028699 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.74523635 -5.64662406 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.05208917 -5.64662406 -6.05208917 -6.74523635 -6.05208917\n",
      " -5.35894199 -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.74523635\n",
      " -5.64662406 -5.35894199 -6.05208917 -4.95347688 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -5.64662406 -6.74523635 -5.64662406 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.74523635 -6.74523635 -5.64662406 -6.05208917 -6.05208917 -6.74523635\n",
      " -5.64662406 -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.05208917 -5.13579844 -6.74523635 -6.05208917 -6.05208917 -6.74523635\n",
      " -5.35894199 -6.05208917 -4.95347688 -6.74523635 -4.95347688 -6.74523635\n",
      " -6.74523635 -6.74523635 -6.74523635 -6.05208917 -5.64662406 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -5.35894199 -6.74523635 -6.05208917 -5.64662406 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.05208917 -5.64662406 -5.35894199 -4.7993262  -6.74523635\n",
      " -6.05208917 -4.18028699 -6.74523635 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.74523635 -4.54801177 -6.05208917 -5.13579844 -5.64662406 -6.05208917\n",
      " -5.64662406 -5.35894199 -6.74523635 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.05208917 -5.35894199 -6.74523635 -6.05208917 -6.74523635\n",
      " -5.64662406 -4.03718615 -6.05208917 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.05208917 -5.64662406 -6.05208917 -6.74523635 -6.74523635 -6.05208917\n",
      " -6.05208917 -4.34734108 -6.05208917 -6.05208917 -5.13579844 -4.7993262\n",
      " -6.74523635 -5.35894199 -6.74523635 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.74523635 -5.64662406 -6.74523635 -4.7993262  -6.74523635\n",
      " -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.05208917 -6.05208917 -5.64662406 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.05208917 -6.74523635 -6.74523635\n",
      " -5.64662406 -6.74523635 -6.05208917 -6.74523635 -5.64662406 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.05208917 -5.35894199 -6.05208917 -6.05208917\n",
      " -5.64662406 -6.74523635 -5.13579844 -6.74523635 -6.74523635 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.74523635 -6.05208917 -4.95347688 -5.64662406\n",
      " -6.05208917 -6.74523635 -5.35894199 -5.13579844 -6.74523635 -5.64662406\n",
      " -6.74523635 -6.05208917 -6.74523635 -6.74523635 -6.74523635 -6.05208917\n",
      " -5.35894199 -6.74523635 -6.05208917 -5.64662406 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.05208917 -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.74523635 -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.05208917\n",
      " -6.05208917 -5.35894199 -6.74523635 -6.74523635 -5.64662406 -6.74523635\n",
      " -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.05208917 -6.74523635\n",
      " -6.05208917 -6.74523635 -6.05208917 -6.74523635 -6.05208917 -6.05208917\n",
      " -6.74523635 -6.05208917 -6.05208917 -6.05208917 -6.74523635 -6.74523635\n",
      " -6.05208917 -6.05208917 -6.05208917 -5.64662406] \n",
      " pAb 0.03025936599423631\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -170.22084368210605 ----p0 -191.88138065021502\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -152.4954343357309 ----p0 -140.81176972234186\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -144.3546376609833 ----p0 -127.75116010064306\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -126.47372394117322 ----p0 -119.8004040780119\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -101.28487469282945 ----p0 -120.75183374102065\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -96.69599478966404 ----p0 -88.63323695253183\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -108.15097746908411 ----p0 -98.71867349353123\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -220.52549356215238 ----p0 -238.38189839782095\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -118.09216304903993 ----p0 -107.17284237903641\n",
      "classify test type <class 'numpy.ndarray'> p0V test <class 'numpy.ndarray'>\n",
      "p1 -241.9519539076908 ----p0 -293.96320861027584\n",
      "error ratio is 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    span_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_most_freq(vocabList,full_list):\n",
    "    vocabCount={}\n",
    "    for word in full_list:\n",
    "        if word in vocabList:\n",
    "            #vocabCount[vocabList.index(word)]+=1\n",
    "            vocabCount[word]=full_list.count(word)\n",
    "    import collections\n",
    "    #print('vocabCount',vocabCount)  \n",
    "    vocab=[]\n",
    "    #vocab=dict(Counter(full_list).most_common(30))\n",
    "    top30=sorted(vocabCount.items(),key=operator.itemgetter(1),reverse=True)[0:30]\n",
    "    #print(type(vocab))\n",
    "    #print(type(top30))\n",
    "    print('top30',top30)\n",
    "    #print('vocab',vocab)\n",
    "    return top30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_word(feed1,feed0):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    doc_list=[]\n",
    "    class_list=[]\n",
    "    full_list=[]\n",
    "    \n",
    "    #1 找出两个中最小的一个\n",
    "    minVal=np.min(len(feed1),len(feed0))\n",
    "    for i in range(min_len):\n",
    "        # 类别　１\n",
    "        word_list = text_parse(feed1['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        # 类别　０\n",
    "        word_list = text_parse(feed0['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    vocabList=createVocabList(doc_list)\n",
    "    \n",
    "    # 去掉高频词\n",
    "    top30Word=calc_most_freq(vocabList,full_text)\n",
    "    #print(top30Word)\n",
    "    for item in top30Word:\n",
    "        if item[0] in vocabList:\n",
    "            vocabList.remove(item[0])\n",
    "    #随机取数训练\n",
    "    traingSet=range(50)\n",
    "    testIndex=[int(num) for num in random.uniform(range(50),10)]\n",
    "    trainIndex=list(set(trainSet)-set(testIndex))\n",
    "    trainMat=[]\n",
    "    trainClass=[]\n",
    "    for i in trainIndex:\n",
    "        \n",
    "        trainMat.append(bag_words2vec(vocabList,doc_list[i]))\n",
    "        trainClass.append(class_list[i])\n",
    "    p0V,p1V,pAb=trainB0(np.array(trainMat),np.array(trainClass))\n",
    "    for i in testIndex:\n",
    "        test=bag_words2vec(vocabList,doc_list[i])\n",
    "        #label=classify(np.array(test),np.array(class_list[i]))\n",
    "        label=classify(np.array(test),p0V,p1V,pAb)\n",
    "        errorCount=0.0\n",
    "        if label!=class_list[i]:\n",
    "            errorCount+=1\n",
    "    #print('error ratio is %s' %(errorCount/len(doc_list)))\n",
    "    print('error ratio is %s' %(errorCount/len(testIndex)))\n",
    "    return vocabList,p0V,p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWord():\n",
    "    import feedparser\n",
    "    \n",
    "    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "    vocabList,p0V,p1V=local_word(ny,sf)\n",
    "    topNy,topSf=[],[]\n",
    "    for i in range(len(pOv)):\n",
    "        if p0V[i]>-6.0:topSf.append(vocabList[i],p0V[i])\n",
    "        if p1V[i]>-6.0:topNy.append(vocabList[i],p1V[i])\n",
    "    sorted_ny=sorted(topNy,key=lambda pair:pair[1],reverse=True)\n",
    "    sorted_sf=sorted(topSf,key=lambda pair:pair[1],reverse=True)\n",
    "    for item in sorted_ny:\n",
    "        print(item[0])\n",
    "    for item in sorted_sf:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
